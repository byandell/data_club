---
title: "Correlation"
output:
  html_document: default
  html_notebook: default
---

```{r libraries, message=FALSE}
library(dplyr)
library(ggplot2)
```

### Learning Objectives

* plot and summarize two phenotypes across groups
* relate two phenotypes using correlation and regression

### Read data

Here we read in data and reduce to species with counts at least 10.

```{r read}
surveys <- read.csv("../data/portal_data_joined.csv")
## Remove missing data.
surveys_complete <- surveys %>%
  filter(species_id != "", !is.na(weight)) %>%
  filter(!is.na(hindfoot_length), sex != "")
# count records per species
species_counts <- surveys_complete %>%
  group_by(species_id) %>%
  tally
# get names of the species with counts >= 10
frequent_species <-  species_counts %>%
  filter(n >= 10) %>%
  select(species_id)
# filter out the less-frequent species
surveys_complete <- surveys_complete %>%
  filter(species_id %in% frequent_species$species_id)
```

Now focus on one genus, `Dipodomys`.

```{r}
surveys_dip <- surveys_complete %>%
  filter(genus == "Dipodomys")
```

### Plot two phenotypes

Now we examine the two phenotypes against each other

```{r}
surveys_plot <- ggplot(surveys_dip, 
                       aes(x = weight, 
                           y = hindfoot_length))
```

```{r}
surveys_plot + 
  geom_point(alpha = 1, shape = 1)
```

### Add a smooth line

```{r}
surveys_plot + 
  geom_point(alpha = 1, shape = 1, col = "gray") +
  geom_smooth()
```

### Correlation Coefficient

In practice we may have many phenotypes to examine, and it is useful to have a summary of how each pair of phenotypes are related. This is called correlation, computed using the `cor()` function. Look at the help page to learn about use.

```{r}
with(surveys_dip, cor(weight, hindfoot_length))
```

Just having one number is somewhat limited. In this case, there is a positive correlation (`r round(with(surveys_dip, cor(weight, hindfoot_length)), 2)`). But how do we interpret it?
The introductory book [_Statisticss_ by Freedman, Pisani and Purvis]() has a nice intrepretation, which is partially presented at [this website](http://www.analytictech.com/mb313/correlat.htm). Basically, if you look at the plot and see an increasing relationship, the correlation is roughly

```
cor = 1 - width / length
```

If the relationship is decreasing, the correlation is the opposite. 

```
cor = -(1 - width / length)
```

Thus you can get a rough idea of correlation with your eye, and over time build up an intuition of how strong correlation is in different settings.

#### Which correlation?

Correlation gives a sense of the relationship, but it depends on the sample size. With more data, a correlation coefficient is better estimated. The idea is that there is some relationship between two phenotypes, which we can estimate in terms of this correlation coefficient. With more data, we get a better estimate.

There are a variety of approaches to measuring correlation. What is generally done is to use the standard, or Pearson, correlation unless there are strong reasons to doubt a linear relationship. In that case the options are to transform the data (as we did above with `log2`) or use an approach that is less sensitive to linearity.

The non-linear approach, an example of non-parametric approaches, measures the degree of monotonic relationship by replacing the data with its ranks. 
The challenge with using measures based on ranks is that we lose some power to detect relationship by making fewer assumptions. Note also that if two phenotypes have the same rank order, then their Spearman correlation is 1, although their Pearson correlation might be somewhat less.

Here are several different ways to compute correlation, each with its own assumptions. They each give slightly different values. Clearly, there are many subtle issues here, and best to consult with a professional if you head down this path.

#### Challenge

Calculate the Spearman correlation, which is based on ranks. How does it compare to Pearson correlation? What type of plot would help illustrate Spearman correlation?
Argue why Pearson correlation may be adequate.

<!-- end challenge -->

<!---
## Pearson, Spearman, rank and Kendall correlation
with(surveys_dip,
     c(pearson  = cor(weight, hindfoot_length),
       rank     = cor(rank(weight), log2(hindfoot_length)),
       spearman = cor(weight, hindfoot_length, method = "spearman"),
       kendall  = cor(weight, hindfoot_length, method = "kendall")))
ggplot(surveys_dip, 
       aes(x=rank(weight), y=rank(hindfoot_length))) +
  geom_point() +
  geom_smooth()
--->

#### Correlation Test

```{r}
with(surveys_dip, cor.test(weight, hindfoot_length))
```

The significance of correlation depends on `df`, which is 2 less than sample size. In this case, the test is highly signficant. There is a confidence interval provided, which you will notice is typically not symmetric about the estimated correlation.

#### Challenge

What are the test statistic ($t$) significance ($p$-value) for this correlation?

<!-- end challenge -->

<!---
tmp <- with(surveys_dip, 
            cor.test(weight, hindfoot_length))
tmp$p.value
tmp$statistic
--->

#### Linear Model interpretation of Correlation

Correlation measures the association between two numeric variables, here `weight` and `hindfoot_length`. One of these variables can be viewed, at least formally, as the predictor and the other as the response. Here, we view `weight` as the predictor of `hindfoot_length`, which we can write in a formula as

```{r eval=FALSE}
hindfoot_length ~ weight
```

We say that we "regress `hindfoot_length` on `weight`". This is sometimes called "simple linear regression", because the right side of this formula can get a lot more complicated (in later lessons). We introduce this concept here to make the bridge to the next lesson on covariates, and to connect correlation to linear models.

Simple linear regression is a special kind of "linear model". The linear model function `lm()` provides another tool to test correlation, with more flexibility than `cor.test()`. `lm` has more bells and whistles and `lm` can handle more complicated relationships.

Put another way, correlation and linear regression are two perspectives on the same relationship. Here we fit a linear model

```{r}
fit <- lm(hindfoot_length ~ weight, surveys_dip)
summary(fit)
```

#### Challenge

While there is a lot of output, notice that the `Coefficients` table has a column for `t value`. Verify that the `t value` for `weight` is identical with the `t` from the `cor.test`. What other terms agree between these to formal summaries?

<!-- end challenge -->

<!---
round(summary(fit)$coefficients["weight","t value"], 2)
--->

### Plot with straight line

We can go back to the plot and use a straight line instead of a smooth one.
Formally, when we measure correlation or simple linear regression, this is the relationship we assume. What is wrong with this picture?

```{r}
surveys_plot + 
  geom_point(alpha = 1, shape = 1) +
  scale_color_brewer(type="qualitative", palette = "Paired") +
  geom_smooth(method="lm")
```

#### Challenge

Earlier, we had used a `sqrt` transformation to adjust for skew in `weight`. Redo above plots and analysis with a `sqrt` transform.

Hint: For plots, you can use `scale_x_sqrt()` to transform the x axis.

<!-- end challenge -->

<!---
surveys_plot + 
  geom_point(alpha = 1, shape = 1) +
  scale_color_brewer(type="qualitative", palette = "Paired") +
  geom_smooth(method="lm") +
  scale_x_sqrt()
with(surveys_dip, cor.test(sqrt(weight), hindfoot_length))
fit_sqrt <- lm(hindfoot_length ~ sqrt(weight), surveys_dip)
summary(fit_sqrt)
--->